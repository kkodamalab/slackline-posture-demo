<!DOCTYPE html>
<html lang="ja">
<head>
<meta charset="UTF-8" />
<title>Posture Demo - MediaPipe</title>
<style>
  body { margin: 0; background: #111; color: #fff; font-family: sans-serif; }
  #container { position: relative; width: 100vw; height: 100vh; overflow: hidden; }
  video, canvas { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }
  #info {
    position: absolute; bottom: 10px; left: 10px;
    background: rgba(0,0,0,0.4); padding: 10px;
    border-radius: 6px; font-size: 18px;
  }
</style>
</head>
<body>

<div id="container">
  <video id="video" autoplay playsinline muted></video>
  <canvas id="canvas"></canvas>
  <div id="info">Loading…</div>
</div>

<!-- MediaPipe Tasks Core / Vision -->
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-core@0.10.0"></script>
<script src="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0"></script>

<script>
(async () => {
  const video  = document.getElementById("video");
  const canvas = document.getElementById("canvas");
  const ctx    = canvas.getContext("2d");
  const info   = document.getElementById("info");

  // カメラ起動
  const stream = await navigator.mediaDevices.getUserMedia({
    video: { facingMode: "user", width: 720, height: 1280 },
    audio: false
  });
  video.srcObject = stream;

  // MediaPipe 初期化
  const vision = await FilesetResolver.forVisionTasks(
    "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm"
  );

  const poseLandmarker = await PoseLandmarker.createFromOptions(vision, {
    baseOptions: {
      modelAssetPath:
        "https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_full/float16/1/pose_landmarker_full.task"
    },
    runningMode: "VIDEO",
    numPoses: 1
  });

  info.textContent = "Pose loaded.";

  let lastVideoTime = -1;

  function renderLoop() {
    if (video.readyState < 2) {
      requestAnimationFrame(renderLoop);
      return;
    }

    // ビデオサイズに合わせてキャンバスを更新
    if (canvas.width !== video.videoWidth) {
      canvas.width  = video.videoWidth;
      canvas.height = video.videoHeight;
    }

    // フレーム更新時のみ推定を実行（公式ガイド準拠）
    // https://ai.google.dev/edge/mediapipe/solutions/vision/pose_landmarker/web_js
    if (video.currentTime === lastVideoTime) {
      requestAnimationFrame(renderLoop);
      return;
    }
    lastVideoTime = video.currentTime;

    const now = performance.now();
    const result = poseLandmarker.detectForVideo(video, now);

    // 背景としてビデオを描画
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

    const midX = canvas.width / 2;
    ctx.strokeStyle = "yellow";
    ctx.lineWidth   = 3;
    ctx.beginPath();
    ctx.moveTo(midX, 0);
    ctx.lineTo(midX, canvas.height);
    ctx.stroke();

    if (!result.landmarks || result.landmarks.length === 0) {
      info.textContent = "Detecting...";
      requestAnimationFrame(renderLoop);
      return;
    }

    info.textContent = "OK";

    const lm = result.landmarks[0];   // 33点の配列

    function avg(a, b) {
      return { x: (a.x + b.x) / 2, y: (a.y + b.y) / 2 };
    }

    // 肩(11,12)と腰(23,24)の中点
    const s = avg(lm[11], lm[12]);
    const h = avg(lm[23], lm[24]);

    s.x *= canvas.width;  s.y *= canvas.height;
    h.x *= canvas.width;  h.y *= canvas.height;

    ctx.strokeStyle = "cyan";
    ctx.lineWidth   = 4;
    ctx.beginPath();
    ctx.moveTo(s.x, s.y);
    ctx.lineTo(h.x, h.y);
    ctx.stroke();

    const deviation = Math.abs(s.x - midX);
    info.textContent = `Deviation: ${deviation.toFixed(1)} px`;

    requestAnimationFrame(renderLoop);
  }

  renderLoop();
})();
</script>

</body>
</html>
